{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataloader import load_data\n",
    "from evaluate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"ticks\")\n",
    "dark_palette = np.array(sns.color_palette(\"dark\", 8))[np.array([0,2,1,3,4,5,6,7]),:]\n",
    "deep_palette = np.array(sns.color_palette(\"deep\", 8))[np.array([0,2,1,3,4,5,6,7]),:]\n",
    "sns.set_palette(dark_palette)\n",
    "plt_colors = sns.color_palette()\n",
    "mpl.rcParams[\"xtick.direction\"] = \"in\"\n",
    "mpl.rcParams[\"ytick.direction\"] = \"in\"\n",
    "mpl.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, w, t, cens = {}, {}, {}, {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets_ml, buckets_conv, arrs_ml, arrs_conv = {}, {}, {}, {}\n",
    "pvals_ml, pvals_conv = {}, {}\n",
    "framingham = {}\n",
    "ascvd = {}\n",
    "pred_rr_ml = {}\n",
    "pred_rr_conv = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in (\"combined\", \"sprint\", \"accord\"):\n",
    "  \n",
    "  X[dataset] = np.load(\"results/xlearner/%s/X.npy\" % dataset)\n",
    "  w[dataset] = np.load(\"results/xlearner/%s/w.npy\" % dataset)\n",
    "  y[dataset] = np.load(\"results/xlearner/%s/y.npy\" % dataset)\n",
    "  t[dataset] = np.load(\"results/xlearner/%s/t.npy\" % dataset)\n",
    "  cens[dataset] = np.load(\"results/xlearner/%s/cens.npy\" % dataset)\n",
    "  \n",
    "  framingham[dataset] = np.load(\"results/baselines/%s/framingham.npy\" % dataset)\n",
    "  ascvd[dataset] = np.load(\"results/baselines/%s/ascvd.npy\" % dataset)\n",
    "  \n",
    "  pred_rr_ml[dataset] = np.load(\"results/xlearner/%s/pred_rr.npy\" % dataset)\n",
    "  pred_rr_conv[dataset] = np.load(\"results/logreg/%s/pred_rr.npy\" % dataset)\n",
    "  \n",
    "  buckets_ml[dataset] = np.load(\"results/xlearner/%s/buckets.npy\" % dataset)\n",
    "  arrs_ml[dataset] = np.load(\"results/xlearner/%s/arrs.npy\" % dataset).item()\n",
    "  pvals_ml[dataset] = np.load(\"results/xlearner/%s/pvals.npy\" % dataset)\n",
    "  buckets_conv[dataset] = np.load(\"results/logreg/%s/buckets.npy\" % dataset)\n",
    "  arrs_conv[dataset] = np.load(\"results/logreg/%s/arrs.npy\" % dataset).item()\n",
    "  pvals_conv[dataset] = np.load(\"results/logreg/%s/pvals.npy\" % dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_corr_in_pred_rr(dataset):\n",
    "  print(stats.pearsonr(pred_rr_ml[dataset], pred_rr_conv[dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_corr_in_pred_rr(\"combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forest_plot(dataset):\n",
    "  n_trials = len(arrs_ml[dataset][BENEFIT_ASSIGNMENT])\n",
    "  buckets, risk_reductions, lengths = [], [], []\n",
    "  buckets += [1] * n_trials\n",
    "  risk_reductions += arrs_ml[dataset][BENEFIT_ASSIGNMENT]\n",
    "  lengths.append(np.sum(buckets_ml[dataset] == BENEFIT_ASSIGNMENT))\n",
    "  buckets += [2] * n_trials\n",
    "  risk_reductions += arrs_conv[dataset][BENEFIT_ASSIGNMENT]\n",
    "  lengths.append(np.sum(buckets_conv[dataset] == BENEFIT_ASSIGNMENT))\n",
    "  buckets += [4] * n_trials\n",
    "  risk_reductions += arrs_ml[dataset][NO_BENEFIT_ASSIGNMENT]\n",
    "  lengths.append(np.sum(buckets_ml[dataset] == NO_BENEFIT_ASSIGNMENT))\n",
    "  buckets += [5] * n_trials\n",
    "  risk_reductions += arrs_conv[dataset][NO_BENEFIT_ASSIGNMENT]\n",
    "  lengths.append(np.sum(buckets_conv[dataset] == NO_BENEFIT_ASSIGNMENT))\n",
    "  buckets = np.array(buckets)\n",
    "  risk_reductions = np.array(risk_reductions)\n",
    "  ytick_labels = [\n",
    "    \"$\\mathbf{Benefit}$\",\n",
    "    \"    Machine Learning\",\n",
    "    \"    Conventional\",\n",
    "    \"$\\mathbf{No\\ benefit}$   \",\n",
    "    \"    Machine Learning\", \n",
    "    \"    Conventional\"]\n",
    "  ytick_locs = [-0, -1,-1.5,-3,-4,-4.5]\n",
    "  fig = plt.figure(figsize=(12, 4), tight_layout=True)\n",
    "  pvals = {\n",
    "    1: pvals_ml[dataset][0],\n",
    "    2: pvals_conv[dataset][0],\n",
    "    4: pvals_ml[dataset][1],\n",
    "    5: pvals_conv[dataset][1]\n",
    "  }\n",
    "  minsofar, maxsofar = 0, 0\n",
    "  for i, b in enumerate([1,2,4,5]):\n",
    "    rng = get_range(risk_reductions[buckets == b])\n",
    "    minsofar = min(minsofar, np.min(rng))\n",
    "    maxsofar = max(maxsofar, np.max(rng))\n",
    "    plt.plot(rng, [ytick_locs[b]] * 3, \"-|\", \n",
    "             color=plt_colors[0] if b == 1 or b == 4 else plt_colors[1])\n",
    "    plt.plot([rng[1]], ytick_locs[b], \"D\", \n",
    "             color=plt_colors[0] if b == 1 or b == 4 else plt_colors[1])\n",
    "  for i, b in enumerate([1,2,4,5]):\n",
    "    rng = get_range(risk_reductions[buckets == b])\n",
    "    if pvals[b] < 0.01:\n",
    "        plt.text(maxsofar + 0.01, ytick_locs[b], \n",
    "                 \"{:.4f} [{:.4f} {:.4f}], $P < 0.01$, $N$ = {}\".format(\n",
    "                   rng[1], rng[0], rng[2], lengths[i]))\n",
    "    else:\n",
    "        plt.text(maxsofar + 0.01, ytick_locs[b], \n",
    "                 \"{:.4f} [{:.4f} {:.4f}], $P$ = {:.2f}, $N$ = {}\".format(\n",
    "                   rng[1], rng[0], rng[2], pvals[b], lengths[i]))\n",
    "  ax = fig.get_axes()[0]\n",
    "  ax.set_yticks([-0, -1,-1.5,-3,-4,-4.5])\n",
    "  r = ax.set_yticklabels(ytick_labels, ha = 'left')\n",
    "  plt.draw()\n",
    "  ax.tick_params(axis=u'y', which=u'both',length=0)\n",
    "  yax = ax.get_yaxis()\n",
    "  pad = max(T.label.get_window_extent().width + 5 for T in yax.majorTicks)\n",
    "  yax.set_tick_params(pad=pad)\n",
    "  plt.xlabel(\"Average Risk Reduction\")\n",
    "  plt.xlim([-0.08, 0.16])\n",
    "  plt.ylim([-5.9,0.5])\n",
    "  plt.axvline(x=0.0, linestyle=\"--\", color=\"grey\")\n",
    "  plt.savefig(\"./paper/img/{}_forest_plot.pdf\".format(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_plot(\"sprint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expected_vs_obs_rr(dataset, n_bins=5, cens_time=365.25 * 3, bin_strategy=\"rr\"):\n",
    "  plt.figure(figsize=(12, 4))\n",
    "  plt.subplot(1,2,1)\n",
    "  rss, slope, intercept, pred_rr, obs_rr, = calibration(pred_rr_ml[dataset], \n",
    "                                                        y[dataset],\n",
    "                                                        w[dataset], \n",
    "                                                        t[dataset],\n",
    "                                                        cens_time, n_bins=n_bins)\n",
    "  plt.scatter(pred_rr, obs_rr, alpha=0.5, color=plt_colors[0])\n",
    "  abline_values = [slope * i + intercept for i in [-0.15, 0.25]]\n",
    "  plt.plot([-0.15, 0.25], abline_values, '--', color=plt_colors[0])\n",
    "  plt.title(\"Machine Learning\", fontsize=12, fontweight=\"bold\")\n",
    "  plt.xlim([-0.15,0.20])\n",
    "  plt.ylim([-0.15,0.20])\n",
    "  plt.text(-0.12, 0.15, \"Slope: {:.2f}, Intercept: {:.2f}\".format(slope, intercept))\n",
    "  plt.xlabel(\"Predicted ARR\")\n",
    "  plt.ylabel(\"Observed ARR\")\n",
    "  plt.plot((-0.3,0.3), (-0.3, 0.3), \"--\", color=\"grey\")\n",
    "  plt.subplot(1,2,2)\n",
    "  rss, slope, intercept, pred_rr, obs_rr, = calibration(pred_rr_conv[dataset], \n",
    "                                                        y[dataset],\n",
    "                                                        w[dataset], \n",
    "                                                        t[dataset],\n",
    "                                                        cens_time, n_bins=n_bins)\n",
    "  rss = np.sum((np.array(obs_rr) - np.array(pred_rr)) ** 2)\n",
    "  plt.scatter(pred_rr, obs_rr, alpha=0.5, color=plt_colors[1])\n",
    "  abline_values = [slope * i + intercept for i in [-0.15, 0.25]]\n",
    "  plt.plot([-0.15, 0.25], abline_values, '--', color=plt_colors[1])\n",
    "  plt.title(\"Conventional\", fontsize=12, fontweight=\"bold\")\n",
    "  plt.xlim([-0.15,0.20])\n",
    "  plt.ylim([-0.15,0.20])\n",
    "  plt.xlabel(\"Predicted ARR\")\n",
    "  plt.ylabel(\"Observed ARR\")\n",
    "  plt.text(-0.12, 0.15, \"Slope: {:.2f}, Intercept: {:.2f}\".format(slope, intercept))\n",
    "  plt.plot((-0.3,0.3), (-0.3, 0.3), \"--\", color=\"grey\")\n",
    "  plt.savefig(\"./paper/img/{}_calibration_curve_by_pred_risk.pdf\".format(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_expected_vs_obs_rr(\"combined\", bin_strategy=\"rr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_rr_against_baseline_decile(dataset, baseline_risk=cox, n_bins=10):\n",
    "  bins = np.percentile(baseline_risk[dataset], q=np.linspace(0, 100, n_bins + 1))\n",
    "  baseline_decile = np.linspace(0, 100, n_bins + 1)[np.digitize(baseline_risk[dataset], bins[:-1]) - 1]\n",
    "  plt.figure(figsize=(10, 4))\n",
    "  plt.axhline(y=0.0, linestyle=\"--\", color=\"grey\")\n",
    "  sns.boxplot(x=np.r_[baseline_decile, baseline_decile] / 10 + 1, \n",
    "              y=np.r_[pred_rr_ml[dataset], pred_rr_conv[dataset]], \n",
    "              hue=np.r_[[\"Machine Learning\"] * len(pred_rr_ml[dataset]), \n",
    "                        [\"Conventional\"] * len(pred_rr_conv[dataset])], \n",
    "              palette=deep_palette,\n",
    "              showfliers=False)\n",
    "  plt.ylabel(\"Predicted ARR\")\n",
    "  plt.xlabel(\"Baseline Risk Decile\")\n",
    "  plt.ylim((-0.15, 0.20))\n",
    "  plt.savefig(\"./paper/img/{}_pred_rr_baseline_decile.pdf\".format(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_rr_against_baseline_decile(\"combined\", baseline_risk=framingham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_baseline_decile_table(dataset, baseline_risk=cox, n_bins=10):\n",
    "  bins = np.percentile(baseline_risk[dataset], q=np.linspace(0, 100, n_bins + 1))\n",
    "  baseline_decile = np.linspace(0, 100, n_bins + 1)[np.digitize(baseline_risk[dataset], bins[:-1]) - 1]\n",
    "  for i in sorted(list(set(baseline_decile))):\n",
    "    u = pred_rr_ml[\"combined\"][baseline_decile == i]\n",
    "    v = pred_rr_conv[\"combined\"][baseline_decile == i]\n",
    "    print(f\"{int(i / 10 + 1)},{np.percentile(u, 50):.4f},{np.percentile(u, 25):.4f},{np.percentile(u, 75):.4f},{np.percentile(v, 50):.4f},{np.percentile(v, 25):.4f},{np.percentile(v, 75):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_baseline_decile_table(\"combined\", baseline_risk=ascvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_rr(dataset):\n",
    "  plt.figure(figsize=(8, 4))\n",
    "  pred_rr = pred_rr_ml[dataset]\n",
    "  sns.kdeplot(pred_rr, label=\"Machine Learning\", shade=True)\n",
    "  pred_rr = pred_rr_conv[dataset]\n",
    "  sns.kdeplot(pred_rr, label=\"Conventional\", shade=True)\n",
    "  plt.ylabel(\"Density\")\n",
    "  plt.xlabel(\"Predicted absolute risk reduction\")\n",
    "  plt.xlim([-0.15, 0.15])\n",
    "  plt.legend()\n",
    "  plt.savefig(\"./paper/img/{}_pred_rr_distributions.pdf\".format(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predicted_rr(\"combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_summary_stats(dataset, bucket=True):\n",
    "  cols = load_data(\"accord\")[\"cols\"]\n",
    "  if bucket:\n",
    "    print(\"== ml [BEN | NOEFF]\")\n",
    "    print(sum(pred_rr_ml[dataset] > 0))\n",
    "    print(sum(pred_rr_ml[dataset] <= 0))\n",
    "    for i, col in enumerate(cols):\n",
    "      ben = X[dataset][:,i][pred_rr_ml[dataset] > 0]\n",
    "      noeff = X[dataset][:,i][pred_rr_ml[dataset] <= 0]\n",
    "      print(\"{}:,{:.2f} ({:.2f}),{:.2f} ({:.2f})\".format(col, ben.mean(), ben.std(),\n",
    "                                                           noeff.mean(), noeff.std()))\n",
    "    print(\"== conv [BEN | NOEFF]\")\n",
    "    print(sum(pred_rr_conv[dataset] > 0))\n",
    "    print(sum(pred_rr_conv[dataset] <= 0))\n",
    "    for i, col in enumerate(cols):\n",
    "      ben = X[dataset][:,i][pred_rr_conv[dataset] > 0]\n",
    "      noeff = X[dataset][:,i][pred_rr_conv[dataset] <= 0]\n",
    "      print(\"{}:,{:.2f} ({:.2f}),{:.2f} ({:.2f})\".format(col, ben.mean(), ben.std(),\n",
    "                                                              noeff.mean(), noeff.std()))\n",
    "  else:\n",
    "    for i, col in enumerate(cols):\n",
    "      print(\"{}:,{:.2f} ({:.2f})\".format(col, X[dataset][:,i].mean(),\n",
    "                                              X[dataset][:,i].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_summary_stats(\"combined\", bucket=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_summary_stats(\"combined\", bucket=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matching_patient_pairs(dataset):\n",
    "  random.seed(1)\n",
    "  plt.figure(figsize=(11, 4))\n",
    "  plt.subplot(1,2,1)\n",
    "  tuples = list(zip(pred_rr_ml[dataset][cens[dataset] == 0], \n",
    "                    y[dataset][cens[dataset] == 0], \n",
    "                    w[dataset][cens[dataset] == 0]))\n",
    "  untreated = list(filter(lambda t: t[2] == 0, tuples))\n",
    "  treated = list(filter(lambda t: t[2] == 1, tuples))\n",
    "  if len(treated) < len(untreated):\n",
    "    untreated = random.sample(untreated, len(treated))\n",
    "  if len(untreated) < len(treated):\n",
    "    treated = random.sample(treated, len(untreated))\n",
    "  assert len(untreated) == len(treated)\n",
    "  untreated = sorted(untreated, key=lambda t: t[0])\n",
    "  treated = sorted(treated, key=lambda t: t[0])\n",
    "  plt.scatter(np.array(treated)[:,0], np.array(untreated)[:,0], marker=\".\", alpha=1e-2)\n",
    "  plt.plot((-0.3, 0.3), (-0.3, 0.3), \"--\", color=\"grey\")\n",
    "  plt.xlabel(\"Predicted ARR, intensive arm\")\n",
    "  plt.ylabel(\"Predicted ARR, standard arm\")\n",
    "  plt.ylim(-0.3, 0.3)\n",
    "  plt.xlim(-0.3, 0.3)\n",
    "  plt.title(\"Machine Learning\", fontsize=12, fontweight=\"bold\")\n",
    "  plt.subplot(1,2,2)\n",
    "  tuples = list(zip(pred_rr_conv[dataset], y[dataset], w[dataset]))\n",
    "  untreated = list(filter(lambda t: t[2] == 0, tuples))\n",
    "  treated = list(filter(lambda t: t[2] == 1, tuples))\n",
    "  if len(treated) < len(untreated):\n",
    "    untreated = random.sample(untreated, len(treated))\n",
    "  if len(untreated) < len(treated):\n",
    "    treated = random.sample(treated, len(untreated))\n",
    "  assert len(untreated) == len(treated)\n",
    "  untreated = sorted(untreated, key=lambda t: t[0])\n",
    "  treated = sorted(treated, key=lambda t: t[0])\n",
    "  plt.scatter(np.array(treated)[:,0], np.array(untreated)[:,0], marker=\".\", alpha=2e-3, color=plt_colors[1])\n",
    "  plt.plot((-0.3, 0.3), (-0.3, 0.3), \"--\", color=\"grey\")\n",
    "  plt.xlabel(\"Predicted ARR, intensive arm\")\n",
    "  plt.ylabel(\"Predicted ARR, standard arm\")\n",
    "  plt.title(\"Conventional\", fontsize=12, fontweight=\"bold\")\n",
    "  plt.ylim(-0.3, 0.3)\n",
    "  plt.xlim(-0.3, 0.3)\n",
    "  plt.savefig(\"./paper/img/{}_matching_patient_pairs.pdf\".format(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matching_patient_pairs(\"combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
